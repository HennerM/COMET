<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Frequently Asked Questions &mdash; COMET 2.0.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/comet.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="COMET Metrics" href="models.html" />
    <link rel="prev" title="Running COMET" href="running.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            COMET
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="running.html">Running COMET</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Frequently Asked Questions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#interpreting-scores">Interpreting Scores:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#which-comet-model-should-i-use">Which COMET model should I use?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#where-can-i-find-the-data-used-to-train-comet-models">Where can I find the data used to train COMET models?</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#direct-assessments">Direct Assessments</a></li>
<li class="toctree-l3"><a class="reference internal" href="#direct-assessments-relative-ranks">Direct Assessments: Relative Ranks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#direct-assessment-scalar-quality-metric">Direct Assessment + Scalar Quality Metric:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multidimensional-quality-metrics">Multidimensional Quality Metrics:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="models.html">COMET Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html#available-evaluation-models">Available Evaluation Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="training.html">Train your own Metric</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">COMET</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Frequently Asked Questions</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/faqs.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="frequently-asked-questions">
<h1>Frequently Asked Questions<a class="headerlink" href="#frequently-asked-questions" title="Permalink to this heading">ÔÉÅ</a></h1>
<p>Since we released COMET we have received several questions related to interpretabilty of the scores and usage. In this section we try to address these questions the best we can!</p>
<section id="interpreting-scores">
<h2>Interpreting Scores:<a class="headerlink" href="#interpreting-scores" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>When using COMET to evaluate machine translation, it‚Äôs important to understand how to interpret the scores it produces.</p>
<p>In general, COMET models are trained to predict quality scores for translations. These scores are typically normalized using a <a class="reference external" href="https://simplypsychology.org/z-score.html">z-score transformation</a> to account for individual differences among annotators. While the raw score itself does not have a direct interpretation, it is useful for ranking translations and systems according to their quality.</p>
<p>However, for the latest COMET models like <a class="reference external" href="https://huggingface.co/Unbabel/wmt22-comet-da"><code class="docutils literal notranslate"><span class="pre">Unbabel/wmt22-comet-da</span></code></a>, we have introduced a new training approach that scales the scores between 0 and 1. This makes it easier to interpret the scores: a score close to 1 indicates a high-quality translation, while a score close to 0 indicates a translation that is no better than random chance.</p>
<p>It‚Äôs worth noting that when using COMET to compare the performance of two different translation systems, it‚Äôs important to run the <code class="docutils literal notranslate"><span class="pre">comet-compare</span></code> command to obtain statistical significance measures. This command compares the output of two systems using a statistical hypothesis test, providing an estimate of the probability that the observed difference in scores between the systems is due to chance. This is an important step to ensure that any differences in scores between systems are statistically significant.</p>
<p>Overall, the added interpretability of scores in the latest COMET models, combined with the ability to assess statistical significance between systems using <code class="docutils literal notranslate"><span class="pre">comet-compare</span></code>, make COMET a valuable tool for evaluating machine translation.</p>
</section>
<section id="which-comet-model-should-i-use">
<h2>Which COMET model should I use?<a class="headerlink" href="#which-comet-model-should-i-use" title="Permalink to this heading">ÔÉÅ</a></h2>
<p><strong>For general purpose MT evaluation</strong> we recommend you to use <code class="docutils literal notranslate"><span class="pre">Unbabel/wmt22-comet-da</span></code>. This is the most <em>stable</em> model we have. It is an improved version of our previous model <code class="docutils literal notranslate"><span class="pre">Unbabel/wmt20-comet-da</span></code>.</p>
<p><strong>For evaluating models without a reference</strong> we recommend the <code class="docutils literal notranslate"><span class="pre">Unbabel/wmt20-comet-qe-da</span></code> for higher correlations with DA, and to <a class="reference external" href="https://github.com/Unbabel/COMET/blob/master/MODELS.md">download <code class="docutils literal notranslate"><span class="pre">wmt21-comet-qe-mqm</span></code></a> for higher correlations with MQM.</p>
</section>
<section id="where-can-i-find-the-data-used-to-train-comet-models">
<h2>Where can I find the data used to train COMET models?<a class="headerlink" href="#where-can-i-find-the-data-used-to-train-comet-models" title="Permalink to this heading">ÔÉÅ</a></h2>
<section id="direct-assessments">
<h3>Direct Assessments<a class="headerlink" href="#direct-assessments" title="Permalink to this heading">ÔÉÅ</a></h3>
<table border="1" class="docutils">
<thead>
<tr>
<th style="text-align: center;">year</th>
<th style="text-align: center;">data</th>
<th style="text-align: center;">paper</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2017</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/comet/data/2017-da.tar.gz">üîó</a></td>
<td style="text-align: center;"><a href="https://aclanthology.org/W17-4717.pdf">Findings of the 2017 Conference on Machine Translation (WMT17)</a></td>
</tr>
<tr>
<td style="text-align: center;">2018</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/comet/data/2018-da.tar.gz">üîó</a></td>
<td style="text-align: center;"><a href="https://aclanthology.org/W18-6401.pdf">Findings of the 2018 Conference on Machine Translation (WMT18)</a></td>
</tr>
<tr>
<td style="text-align: center;">2019</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/comet/data/2019-da.tar.gz">üîó</a></td>
<td style="text-align: center;"><a href="https://aclanthology.org/W19-5301.pdf">Findings of the 2019 Conference on Machine Translation (WMT19)</a></td>
</tr>
<tr>
<td style="text-align: center;">2020</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/comet/data/2020-da.tar.gz">üîó</a></td>
<td style="text-align: center;"><a href="https://aclanthology.org/2020.wmt-1.1.pdf">Findings of the 2020 Conference on Machine Translation (WMT20)</a></td>
</tr>
<tr>
<td style="text-align: center;">2021</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/comet/data/2021-da.tar.gz">üîó</a></td>
<td style="text-align: center;"><a href="https://aclanthology.org/2021.wmt-1.1.pdf">Findings of the 2021 Conference on Machine Translation (WMT21)</a></td>
</tr>
<tr>
<td style="text-align: center;">2022</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/comet/data/2022-da.tar.gz">üîó</a></td>
<td style="text-align: center;"><a href="https://aclanthology.org/2022.wmt-1.1.pdf">Findings of the 2022 Conference on Machine Translation (WMT22)</a></td>
</tr>
</tbody>
</table><p>Another large source of DA annotations is the <a class="reference external" href="https://aclanthology.org/2022.lrec-1.530.pdf">MLQE-PE corpus</a> that is typically used for quality estimation shared tasks <a class="reference external" href="https://aclanthology.org/2020.wmt-1.79.pdf">(Specia et al. 2020</a><a class="reference external" href="https://aclanthology.org/2021.wmt-1.71.pdf">, 2021</a><a class="reference external" href="https://aclanthology.org/2022.wmt-1.3.pdf">; Zerva et al. 2022)</a>.</p>
<p>You can download MLQE-PE by using the following <a class="reference external" href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/comet/data/mlqe-pe.tar.gz">üîó</a>.</p>
</section>
<section id="direct-assessments-relative-ranks">
<h3>Direct Assessments: Relative Ranks<a class="headerlink" href="#direct-assessments-relative-ranks" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>Before 2021 the WMT Metrics shared task used relative ranks to evaluate metrics.</p>
<p>Relative ranks can be created when we have at least two DA scores for translations of the same source input, by converting those DA scores into a relative ranking judgement, if the difference in DA scores allows conclusion that one translation is better than the other (usually atleast 25 points).</p>
<p>To make it easier to replicate results from previous Metrics shared tasks (2017-2020) you can find the preprocessed DA relative ranks in the table below:</p>
<table border="1" class="docutils">
<thead>
<tr>
<th style="text-align: center;">year</th>
<th style="text-align: center;">relative ranks</th>
<th style="text-align: center;">paper</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2017</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/wmt/2017-daRR.csv.tar.gz">üîó</a></td>
<td style="text-align: center;"><a href="https://statmt.org/wmt17/pdf/WMT55.pdf">Results of the WMT17 Metrics Shared Task</a></td>
</tr>
<tr>
<td style="text-align: center;">2018</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/wmt/2018-daRR.csv.tar.gz">üîó</a></td>
<td style="text-align: center;"><a href="https://statmt.org/wmt18/pdf/WMT078.pdf">Results of the WMT18 Metrics Shared Task</a></td>
</tr>
<tr>
<td style="text-align: center;">2019</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/wmt/2019-daRR.csv.tar.gz">üîó</a></td>
<td style="text-align: center;"><a href="https://statmt.org/wmt19/pdf/53/WMT02.pdf">Results of the WMT19 Metrics Shared Task</a></td>
</tr>
<tr>
<td style="text-align: center;">2020</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/wmt/2020-daRR.csv.tar.gz">üîó</a></td>
<td style="text-align: center;"><a href="https://aclanthology.org/2020.wmt-1.77.pdf">Results of the WMT20 Metrics Shared Task</a></td>
</tr>
</tbody>
</table></section>
<section id="direct-assessment-scalar-quality-metric">
<h3>Direct Assessment + Scalar Quality Metric:<a class="headerlink" href="#direct-assessment-scalar-quality-metric" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>In 2022, several changes were made to the annotation procedure used in the WMT Translation task. In contrast to the standard DA (sliding scale from 0-100) used in previous years, in 2022 annotators performed DA+SQM (Direct Assessment + Scalar Quality Metric). In DA+SQM, the annotators still provide a raw score between 0 and 100, but also are presented with seven labeled tick marks. DA+SQM helps to stabilize scores across annotators (as compared to DA).</p>
<table border="1" class="docutils">
<thead>
<tr>
<th style="text-align: center;">year</th>
<th style="text-align: center;">data</th>
<th style="text-align: center;">paper</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2022</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/comet/data/2022-sqm.tar.gz">üîó</a></td>
<td style="text-align: center;"><a href="https://aclanthology.org/2022.wmt-1.1.pdf">Findings of the 2022 Conference on Machine Translation (WMT22)</a></td>
</tr>
</tbody>
</table></section>
<section id="multidimensional-quality-metrics">
<h3>Multidimensional Quality Metrics:<a class="headerlink" href="#multidimensional-quality-metrics" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>Since 2021 the WMT Metrics task decided to perform they own expert-based evaluation based on <em>Multidimensional Quality Metrics (MQM)</em> framework. In the table below you can find MQM annotations from previous years.</p>
<table border="1" class="docutils">
<thead>
<tr>
<th style="text-align: center;">year</th>
<th style="text-align: center;">data</th>
<th style="text-align: center;">paper</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2020</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/comet/data/2020-mqm.tar.gz">üîó</a></td>
<td style="text-align: center;"><a href="https://aclanthology.org/2021.tacl-1.87.pdf">A Large-Scale Study of Human Evaluation for Machine Translation</a></td>
</tr>
<tr>
<td style="text-align: center;">2021</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/comet/data/2021-mqm.tar.gz">üîó</a></td>
<td style="text-align: center;"><a href="https://aclanthology.org/2021.wmt-1.73.pdf">Results of the WMT21 Metrics Shared Task</a></td>
</tr>
<tr>
<td style="text-align: center;">2022</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/comet/data/2022-mqm.tar.gz">üîó</a></td>
<td style="text-align: center;"><a href="https://aclanthology.org/2022.wmt-1.2.pdf">Results of the WMT22 Metrics Shared Task</a></td>
</tr>
</tbody>
</table><p><strong>Note:</strong> You can find the original MQM data <a class="reference external" href="https://github.com/google/wmt-mqm-human-evaluation">here</a>.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="running.html" class="btn btn-neutral float-left" title="Running COMET" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="models.html" class="btn btn-neutral float-right" title="COMET Metrics" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Unbabel. All rights reserved.Source code available under Apache License 2.0.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>